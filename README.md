# ğŸŒ³ **Decision Trees & Random Forests**
## â¤ï¸ **Heart Disease Prediction using Machine Learning**

---

## ğŸ¯ **Objective**
The objective of this project is to **learn, implement, and analyze tree-based machine learning models** for **classification tasks** using the **UCI Heart Disease Dataset**.

This project focuses on:
- **Training & visualizing a Decision Tree Classifier**
- **Analyzing overfitting and controlling tree depth**
- **Training a Random Forest Classifier and comparing accuracy**
- **Interpreting feature importance**
- **Evaluating model performance using cross-validation**

---

## ğŸ“Š **Dataset**
### **Heart Disease Dataset (UCI)**

- Patient medical data with a **binary target variable**
- **Target Labels:**
  - `0` â†’ No Heart Disease
  - `1` â†’ Presence of Heart Disease
- **Key Features:**
  - **age**, **sex**, **cp (chest pain type)**
  - **chol (cholesterol)**, **trestbps**
  - **thalach (maximum heart rate)**, **oldpeak**

ğŸ“ **Dataset Location**

---

## ğŸ› ï¸ **Environment Setup**
The project was developed in a **Conda environment** named **`aiml`**.

### ğŸ”§ **Required Libraries**
```bash
conda install pandas numpy matplotlib seaborn scikit-learn graphviz ipykernel jupyter -y

import pandas as pd
df = pd.read_csv("week 5/heart.csv")
df.head()


ğŸŒ³ 2. Decision Tree Classifier

Split data into 80% training and 20% testing

Trained a Decision Tree Classifier

Evaluated using:

Accuracy Score

Confusion Matrix

Classification Report

Visualized the tree using:

Matplotlib (plot_tree)

Graphviz (export_graphviz)

ğŸ“‰ 3. Overfitting Analysis

Controlled model complexity using max_depth

Plotted:

Training Accuracy vs Tree Depth

Testing Accuracy vs Tree Depth

Observed overfitting at higher depths

ğŸŒ²ğŸŒ² 4. Random Forest Classifier

Trained a Random Forest Classifier with 100 trees

Compared performance with Decision Tree

Extracted feature importances

Visualized important features using a bar chart

ğŸ” 5. Cross-Validation

Applied 5-Fold Cross-Validation

Compared average accuracy of:

Decision Tree

Random Forest

Confirmed better stability of Random Forest

ğŸ“ˆ Results

(Replace XX with actual values)

ğŸŒ³ Decision Tree Accuracy: XX%

ğŸŒ²ğŸŒ² Random Forest Accuracy: XX%

âœ… Random Forest achieved higher accuracy and stability

ğŸ”‘ Top Influential Features

age

cp (chest pain type)

thalach

oldpeak

ğŸ“Š Visualizations

ğŸ“„ Decision Tree Structure (Graphviz PDF)

ğŸ“‰ Train vs Test Accuracy vs Tree Depth

ğŸ“Š Random Forest Feature Importance Plot

ğŸ§  Conclusion

Decision Trees are simple and interpretable but prone to overfitting

Random Forests reduce overfitting and improve prediction accuracy

Feature importance analysis highlights key health indicators

Cross-validation confirms model reliability

ğŸ§° Technologies Used

Python

Pandas & NumPy

Scikit-Learn

Matplotlib & Seaborn

Graphviz

Jupyter Notebook

ğŸ‘¤ Author
Naman Kumar Rai
